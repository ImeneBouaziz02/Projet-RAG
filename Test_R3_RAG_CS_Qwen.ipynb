{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "a72we9sABXuF",
    "outputId": "fb7a15b0-dec0-416c-99de-c7b8faa4321b"
   },
   "outputs": [],
   "source": [
    "# Vérifier les ressources disponibles\n",
    "!nvidia-smi\n",
    "!df -h\n",
    "!free -h\n",
    "\n",
    "# Installation des packages requis\n",
    "!pip install torch transformers accelerate\n",
    "!pip install vllm>=0.2.0\n",
    "!pip install datasets\n",
    "!pip install faiss-cpu  # Pour la recherche vectorielle\n",
    "!pip install sentence-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CkTnpk5qBvOl",
    "outputId": "50a626b4-9e99-4b18-cd23-e210c21e508f"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from google.colab import drive\n",
    "\n",
    "# Monter Google Drive (optionnel, pour sauvegarder)\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Cloner le repository\n",
    "!git clone https://github.com/Yuan-Li-FNLP/R3-RAG.git\n",
    "%cd R3-RAG\n",
    "\n",
    "# Télécharger un modèle plus petit (choisir selon la RAM disponible)\n",
    "print(\"Téléchargement du modèle R3-RAG-CS-Qwen (7B)...\")\n",
    "!git lfs install\n",
    "\n",
    "# Alternative : utiliser un modèle cold start (plus petit)\n",
    "!git clone https://huggingface.co/Yuan-Li-FNLP/R3-RAG-CS-Qwen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_I4pD2P8EyQf",
    "outputId": "ac474a3c-c28e-4dee-dcb1-e9439be78536"
   },
   "outputs": [],
   "source": [
    "MODEL_PATH = \"/content/R3-RAG/R3-RAG-CS-Qwen\"\n",
    "\n",
    "# Vérifier que le modèle existe\n",
    "import os\n",
    "if os.path.exists(MODEL_PATH):\n",
    "    print(f\"✅ Modèle trouvé : {MODEL_PATH}\")\n",
    "    print(f\"Taille : {sum(os.path.getsize(os.path.join(MODEL_PATH, f)) for f in os.listdir(MODEL_PATH) if os.path.isfile(os.path.join(MODEL_PATH, f))) / (1024**3):.2f} GB\")\n",
    "else:\n",
    "    print(f\"❌ Modèle non trouvé : {MODEL_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 118,
     "referenced_widgets": [
      "2b087b491ad349da9a6542c99b51832f",
      "d1ca02cc78544d35bf829f41942cf0a1",
      "0cf4ec047ccb44eda470c96b78963b09",
      "ff452cb12dfa4b73af6cce10b8592869",
      "b3ea6527a38c4fe190950502f155f661",
      "d9e59e3357224f128fe1f9a7ca8c498e",
      "1e85d466508b4f0aaa86f7eb9e0aead9",
      "d18729be3b2b470c94ea1d88d23a6b72",
      "8bc819498eac4af48fb28bcb0351f410",
      "511997bc046a439cac2004ef18f42f02",
      "bc941692a76444babf9b192615c68f8e"
     ]
    },
    "id": "-jTmtLVIE5LV",
    "outputId": "c60cf10f-e109-4b93-8447-3a4edcdff93e"
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "\n",
    "# Charger le tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH)\n",
    "\n",
    "# Charger le modèle avec précision réduite\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_PATH,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "print(\"✅ Modèle chargé avec succès !\")\n",
    "print(f\"Modèle sur device : {model.device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rvAXynsGISyX"
   },
   "source": [
    "30 min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Lakxtc4TFJCC",
    "outputId": "6c6effba-bd7f-4362-9fac-fe7177d2e953"
   },
   "outputs": [],
   "source": [
    "def test_model(question):\n",
    "    # Format de prompt pour R3-RAG\n",
    "    prompt = f\"\"\"Question: {question}\n",
    "\n",
    "Let me think step by step and retrieve relevant information.\n",
    "\n",
    "Action: \"\"\"\n",
    "\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=200,\n",
    "            do_sample=True,\n",
    "            temperature=0.7,\n",
    "            pad_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "\n",
    "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    return response[len(prompt):]\n",
    "\n",
    "# Test avec une question simple\n",
    "question = \"What is the capital of France?\"\n",
    "response = test_model(question)\n",
    "print(f\"Question: {question}\")\n",
    "print(f\"Response: {response}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "h-Y2PHPNmCF3"
   },
   "outputs": [],
   "source": [
    "def test_model(question):\n",
    "    # Format de prompt pour R3-RAG\n",
    "    prompt = f\"\"\"Question: {question}\n",
    "\n",
    "Let me think step by step and retrieve relevant information.\n",
    "\n",
    "Action: \"\"\"\n",
    "\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=200,\n",
    "            do_sample=True,\n",
    "            temperature=0.7,\n",
    "            pad_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "\n",
    "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    return response[len(prompt):]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Uc9023gjIWc9"
   },
   "source": [
    "3h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Exc1O0EYNEM9",
    "outputId": "ea1f36eb-5a47-4496-e698-fa81c0ffe8b4"
   },
   "outputs": [],
   "source": [
    "# Test avec une question multi-hop\n",
    "question = \"Who is the director of the movie that won the Academy Award for Best Picture in 2020?\"\n",
    "response = test_model(question)\n",
    "print(f\"Question: {question}\")\n",
    "print(f\"Response: {response}\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
