{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "512xLEhiefaS"
   },
   "source": [
    "## Test R3-RAG-CS-Qwen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "a72we9sABXuF",
    "outputId": "4b5530ca-ce58-491f-f181-9f4c91c082a0"
   },
   "outputs": [],
   "source": [
    "# Vérifier les ressources disponibles\n",
    "!nvidia-smi\n",
    "!df -h\n",
    "!free -h\n",
    "\n",
    "# Installation des packages requis\n",
    "!pip install torch transformers accelerate\n",
    "!pip install vllm>=0.2.0\n",
    "!pip install datasets\n",
    "!pip install faiss-cpu  # Pour la recherche vectorielle\n",
    "!pip install sentence-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CkTnpk5qBvOl",
    "outputId": "28f8f79c-1438-4f5e-d0a3-91fea1ff144b"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from google.colab import drive\n",
    "\n",
    "# Monter Google Drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Cloner le repository\n",
    "!git clone https://github.com/Yuan-Li-FNLP/R3-RAG.git\n",
    "%cd R3-RAG\n",
    "\n",
    "# Télécharger un modèle plus petit\n",
    "print(\"Téléchargement du modèle R3-RAG-CS-Qwen (7B)...\")\n",
    "!git lfs install\n",
    "\n",
    "!git clone https://huggingface.co/Yuan-Li-FNLP/R3-RAG-CS-Qwen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_I4pD2P8EyQf",
    "outputId": "e8eb08f6-ec40-4311-b1ef-8dcbe9cb024d"
   },
   "outputs": [],
   "source": [
    "MODEL_PATH = \"/content/R3-RAG/R3-RAG-CS-Qwen\"\n",
    "\n",
    "# Vérifier que le modèle existe\n",
    "import os\n",
    "if os.path.exists(MODEL_PATH):\n",
    "    print(f\"✅ Modèle trouvé : {MODEL_PATH}\")\n",
    "    print(f\"Taille : {sum(os.path.getsize(os.path.join(MODEL_PATH, f)) for f in os.listdir(MODEL_PATH) if os.path.isfile(os.path.join(MODEL_PATH, f))) / (1024**3):.2f} GB\")\n",
    "else:\n",
    "    print(f\"❌ Modèle non trouvé : {MODEL_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 118,
     "referenced_widgets": [
      "4f753615417d44d1bd053b94f765dda1",
      "5ffd23d5f3f743ac9529fbbdc18285dd",
      "bf89e04baf4b4e878021a063b8fab716",
      "47ba3c9a54d14b2586532d6c9730b0c5",
      "3a53c81989384a78bb4e4b8eba2f95a8",
      "a572954e380243b995c8031f8a2a4254",
      "426bd758054446c390bd6dcd8792c806",
      "3762edeaff6d4339ad90caa12a713534",
      "00c1de1b3e994cebae5641b9fa6612c6",
      "b3a2c7b478b649babf304957f2dfc11f",
      "3ab0234d4557461b816e7dc86c0aba7a"
     ]
    },
    "id": "-jTmtLVIE5LV",
    "outputId": "d27d9b2f-183e-45b2-e0c8-6e35f3057993"
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "\n",
    "# Charger le tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH)\n",
    "\n",
    "# Charger le modèle avec précision réduite\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_PATH,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "print(\"✅ Modèle chargé avec succès !\")\n",
    "print(f\"Modèle sur device : {model.device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-K3Aw9gmpTWr"
   },
   "outputs": [],
   "source": [
    "def test_model(question):\n",
    "    prompt = f\"\"\"Question: {question}\n",
    "\n",
    "Let me think step by step and retrieve relevant information.\n",
    "\n",
    "Action: \"\"\"\n",
    "\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=100,\n",
    "            do_sample=True,\n",
    "            temperature=0.7,\n",
    "            pad_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "\n",
    "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    return response[len(prompt):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iEScEh5dpb8a",
    "outputId": "8047622e-ec09-4054-bef1-44c8c5e1e572"
   },
   "outputs": [],
   "source": [
    "def run_comprehensive_test(model, tokenizer):\n",
    "    \"\"\"\n",
    "    Test complet du modèle R3-RAG-CS\n",
    "    \"\"\"\n",
    "    test_cases_cold_start = [\n",
    "    {\n",
    "        \"category\": \"Factual - Very Easy\",\n",
    "        \"question\": \"What is the capital of France?\",\n",
    "        \"expected_type\": \"Direct fact\"\n",
    "    },\n",
    "    {\n",
    "        \"category\": \"Factual - Easy\",\n",
    "        \"question\": \"Who invented the light bulb?\",\n",
    "        \"expected_type\": \"Single fact retrieval\"\n",
    "    },\n",
    "    {\n",
    "        \"category\": \"Simple Multi-hop\",\n",
    "        \"question\": \"What ocean is located east of the United States?\",\n",
    "        \"expected_type\": \"Geographic reasoning\"\n",
    "    }\n",
    "]\n",
    "\n",
    "    results = []\n",
    "    for test in test_cases_cold_start:\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"Category: {test['category']}\")\n",
    "        print(f\"Question: {test['question']}\")\n",
    "        print(f\"Expected: {test['expected_type']}\")\n",
    "        print(f\"{'-'*60}\")\n",
    "\n",
    "        response = test_model(test['question'])\n",
    "        print(f\"Response: {response}\")\n",
    "\n",
    "        results.append({\n",
    "            \"question\": test['question'],\n",
    "            \"response\": response,\n",
    "            \"category\": test['category']\n",
    "        })\n",
    "\n",
    "    return results\n",
    "\n",
    "# Exécuter les tests\n",
    "results = run_comprehensive_test(model, tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SWFuRk0iejB1"
   },
   "source": [
    "## Test R3-RAG-Qwen (full)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lseVX1mTenwJ",
    "outputId": "510e668f-6086-4114-aeb5-b5d99c27c8e3"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from google.colab import drive\n",
    "\n",
    "# Monter Google Drive (optionnel, pour sauvegarder)\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Cloner le repository\n",
    "!git clone https://github.com/Yuan-Li-FNLP/R3-RAG.git\n",
    "%cd R3-RAG\n",
    "\n",
    "# Télécharger un modèle plus petit (choisir selon la RAM disponible)\n",
    "print(\"Téléchargement du modèle R3-RAG-CS-Qwen (7B)...\")\n",
    "!git lfs install\n",
    "\n",
    "!git clone https://huggingface.co/Yuan-Li-FNLP/R3-RAG-Qwen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dpcIgW1Ue4T5",
    "outputId": "4b396f40-1ab8-4c8a-ad5f-a4842c1c52c6"
   },
   "outputs": [],
   "source": [
    "MODEL_PATH = \"/content/R3-RAG/R3-RAG-Qwen\"\n",
    "\n",
    "# Vérifier que le modèle existe\n",
    "import os\n",
    "if os.path.exists(MODEL_PATH):\n",
    "    print(f\"✅ Modèle trouvé : {MODEL_PATH}\")\n",
    "    print(f\"Taille : {sum(os.path.getsize(os.path.join(MODEL_PATH, f)) for f in os.listdir(MODEL_PATH) if os.path.isfile(os.path.join(MODEL_PATH, f))) / (1024**3):.2f} GB\")\n",
    "else:\n",
    "    print(f\"❌ Modèle non trouvé : {MODEL_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "N8lwEV8tlNpH"
   },
   "outputs": [],
   "source": [
    "MODEL_PATH = \"/content/R3-RAG/R3-RAG-Qwen\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 118,
     "referenced_widgets": [
      "d1c4d4a031cf4e05850990cb1fc7e72a",
      "67fdaf4a0d8a40189b6e531436a1f5d3",
      "7105569d5c1d4f43bcd45ffa050202d5",
      "55b22096351449e892abcbd18627c943",
      "d3d48eaf078848b59c0311ffea89bbc1",
      "6aaf307359a5496eae18720c9c45d643",
      "222e884d7ada4ae1b95265e55427da5b",
      "61cefef737c340feb4c0d3a43f639047",
      "aacd4f3a305b494d9b4021f0eabebfbe",
      "01f7a07cceb4492e9aa683640fc16948",
      "f62c7d90a71145e7aa45c0f94f7ebc00"
     ]
    },
    "id": "2FNu3VxKfOG4",
    "outputId": "00df8e0e-c870-457f-a500-1fd4d4a0a4a1"
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "\n",
    "# Charger le tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH)\n",
    "\n",
    "# Charger le modèle avec précision réduite\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_PATH,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "print(\"✅ Modèle chargé avec succès !\")\n",
    "print(f\"Modèle sur device : {model.device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "khiKszG3l6-e"
   },
   "outputs": [],
   "source": [
    "def test_model(question):\n",
    "    # Format de prompt pour R3-RAG\n",
    "    prompt = f\"\"\"Question: {question}\n",
    "\n",
    "Let me think step by step and retrieve relevant information.\n",
    "\n",
    "Action: \"\"\"\n",
    "\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=200,\n",
    "            do_sample=True,\n",
    "            temperature=0.7,\n",
    "            pad_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "\n",
    "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    return response[len(prompt):]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7jhmfj5ov_0E"
   },
   "source": [
    "2min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "66Fn6AuOii_l",
    "outputId": "251d387e-c3ce-4c15-ae02-db2d58634d5d"
   },
   "outputs": [],
   "source": [
    "def run_comprehensive_test(model, tokenizer):\n",
    "    \"\"\"\n",
    "    Test complet du modèle R3-RAG\n",
    "    \"\"\"\n",
    "    test_cases = [\n",
    "        {\n",
    "            \"category\": \"Factual - Easy\",\n",
    "            \"question\": \"What is the capital of Japan?\",\n",
    "            \"expected_type\": \"Single fact retrieval\"\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    results = []\n",
    "    for test in test_cases:\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"Category: {test['category']}\")\n",
    "        print(f\"Question: {test['question']}\")\n",
    "        print(f\"Expected: {test['expected_type']}\")\n",
    "        print(f\"{'-'*60}\")\n",
    "\n",
    "        response = test_model(test['question'])\n",
    "        print(f\"Response: {response}\")\n",
    "\n",
    "        results.append({\n",
    "            \"question\": test['question'],\n",
    "            \"response\": response,\n",
    "            \"category\": test['category']\n",
    "        })\n",
    "\n",
    "    return results\n",
    "\n",
    "# Exécuter les tests\n",
    "results = run_comprehensive_test(model, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true,
     "base_uri": "https://localhost:8080/"
    },
    "id": "R5Wp14Kfk9dY",
    "outputId": "07337225-9833-4460-ccae-c98abdaa2fa7"
   },
   "outputs": [],
   "source": [
    "  def run_comprehensive_test(model, tokenizer):\n",
    "    \"\"\"\n",
    "    Test complet du modèle R3-RAG\n",
    "    \"\"\"\n",
    "    test_cases = [\n",
    "\n",
    "        {\n",
    "            \"category\": \"Multi-hop - Hard\",\n",
    "            \"question\": \"Who directed the movie that won the Academy Award for Best Picture in 2020?\",\n",
    "            \"expected_type\": \"Complex retrieval + reasoning\"\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    results = []\n",
    "    for test in test_cases:\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"Category: {test['category']}\")\n",
    "        print(f\"Question: {test['question']}\")\n",
    "        print(f\"Expected: {test['expected_type']}\")\n",
    "        print(f\"{'-'*60}\")\n",
    "\n",
    "        response = test_model(test['question'])\n",
    "        print(f\"Response: {response}\")\n",
    "\n",
    "        results.append({\n",
    "            \"question\": test['question'],\n",
    "            \"response\": response,\n",
    "            \"category\": test['category']\n",
    "        })\n",
    "\n",
    "    return results\n",
    "\n",
    "# Exécuter les tests\n",
    "results = run_comprehensive_test(model, tokenizer)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "SWFuRk0iejB1"
   ],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
